---
language:
- en
- ko
license: apache-2.0
library_name: transformers
base_model:
- meta-llama/Meta-Llama-3-8B
---

---
language:
- en
- ko
license: llama3
library_name: transformers
base_model:
- meta-llama/Meta-Llama-3-8B
---

<a href="https://github.com/MLP-Lab/Bllossom">
  <img src="https://github.com/teddysum/bllossom/blob/main//bllossom_icon.png?raw=true" width="40%" height="50%">
</a>

# Bllossom | [Demo]() | [Homepage](https://www.bllossom.ai/) | [Github](https://github.com/MLP-Lab/Bllossom) | [Colab-tutorial](https://colab.research.google.com/drive/1fBOzUVZ6NRKk_ugeoTbAOokWKqSN47IG?usp=sharing) |

[Latest Update] 
- 2024.05.08 Vocab Expansion model update

The Bllossom language model is a Korean-English bilingual language model based on the open-source LLama3. It enhances the connection of knowledge between Korean and English. It has the following features:

* **Knowledge Linking**: Linking Korean and English knowledge through additional training
* **Vocabulary Expansion**: Expansion of Korean vocabulary to enhance Korean expressiveness.
* **Instruction Tuning**: Tuning using custom-made instruction following data specialized for Korean language and Korean culture
* **Human Feedback**: DPO has been applied
* **Vision-Language Alignment**: Aligning the vision transformer with this language model 

**This model developed by [MLPLab at Seoultech](http://mlp.seoultech.ac.kr), [Teddysum](http://teddysum.ai/) and [Yonsei Univ](https://sites.google.com/view/hansaemkim/hansaem-kim)**

## Demo Video

<div style="display: flex; justify-content: space-between;">
  <!-- ì²« ë²ˆì§¸ ì»¬ëŸ¼ -->
  <div style="width: 49%;">
    <a>
      <img src="https://github.com/lhsstn/lhsstn/blob/main/x-llava_dem.gif?raw=true" style="width: 100%; height: auto;">
    </a>
    <p style="text-align: center;">Bllossom-V Demo</p>
  </div>

  <!-- ë‘ ë²ˆì§¸ ì»¬ëŸ¼ (í•„ìš”í•˜ë‹¤ë©´) -->
  <div style="width: 49%;">
    <a>
      <img src="https://github.com/lhsstn/lhsstn/blob/main/bllossom_demo_kakao.gif?raw=true" style="width: 70%; height: auto;">
    </a>
    <p style="text-align: center;">Bllossom Demo(Kakao)ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤</p>
  </div>
</div>



## NEWS
* [2024/04] We released Bllossom v2.0, based on llama-3
* [2023/12] We released Bllossom-Vision v1.0, based on Bllossom
* [2023/08] We released Bllossom v1.0, based on llama-2. 
* [2023/07] We released Bllossom v0.7, based on polyglot-ko.


```bash
ì €í¬ ì„œìš¸ê³¼ê¸°ëŒ€ MLPì—°êµ¬ì‹¤ì—ì„œ í•œêµ­ì–´-ì˜ì–´ ì´ì¤‘ ì–¸ì–´ëª¨ë¸ì¸ Bllossomì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤!
 - LLama3-8B ê¸°ë°˜ì˜ ê²½ëŸ‰í™”ëœ ì‚¬ì´ì¦ˆ 
 - í•œêµ­ì–´-ì˜ì–´ ì§€ì‹ì—°ê²°ì„ í†µí•œ í•œêµ­ì–´ ì§€ì‹ ê°•í™”
 - í•œêµ­ì–´ ì–´íœ˜ì¶”ê°€
 - í•œêµ­ì–´ ë¬¸í™”, ì–¸ì–´ë¥¼ ê³ ë ¤í•œ ìì²´ì œì‘ ë°ì´í„° ê¸°ë°˜ ë¯¸ì„¸ì¡°ì •
 - ê°•í™”í•™ìŠµ (DPO)
 - ì‹œê°-ì–¸ì–´ ëª¨ë¸í™•ì¥

1. Bllossomì€ ì„œìš¸ê³¼ê¸°ëŒ€, í…Œë””ì¸, ì—°ì„¸ëŒ€ ì–¸ì–´ìì› ì—°êµ¬ì‹¤ì˜ ì–¸ì–´í•™ìì™€ í˜‘ì—…í•´ ë§Œë“  ì‹¤ìš©ì£¼ì˜ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤! ì•ìœ¼ë¡œ ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ë¥¼ í†µí•´ ê´€ë¦¬í•˜ê² ìŠµë‹ˆë‹¤ ë§ì´ í™œìš©í•´ì£¼ì„¸ìš” ğŸ™‚
2. Bllossom70Bëª¨ë¸, ì–´íœ˜í™•ì¥ëª¨ë¸, ì‹œê°-ì–¸ì–´ëª¨ë¸ì€ ì¶”í›„ ê³µê°œí•  ì˜ˆì •ì…ë‹ˆë‹¤. (ê¶ê¸ˆí•˜ì‹ ë¶„ì€ ê°œë³„ ì—°ë½ì£¼ì„¸ìš”, GPUë§Œ ì§€ì›í•´ì£¼ì‹œë©´ ë¬´ë£Œë¡œ ë“œë¦½ë‹ˆë‹¤!)
3. Bllossomì€ NAACL2024, LREC-COLING2024 (êµ¬ë‘) ë°œí‘œë¡œ ì±„íƒë˜ì—ˆìŠµë‹ˆë‹¤.
4. ì¢‹ì€ ì–¸ì–´ëª¨ë¸ ê³„ì† ì—…ë°ì´íŠ¸ í•˜ê² ìŠµë‹ˆë‹¤!! í•œêµ­ì–´ ê°•í™”ë¥¼ìœ„í•´ ê³µë™ ì—°êµ¬í•˜ì‹¤ë¶„ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤!!
```


## Example code

### Colab Tutorial
 - [Inference-Code-Link](https://colab.research.google.com/drive/1fBOzUVZ6NRKk_ugeoTbAOokWKqSN47IG?usp=sharing)

### Install Dependencies
```bash
pip install torch transformers==4.40.0 accelerate
```

### Python code with Pipeline
```python
import transformers
import torch

model_id = "MLP-KTLim/llama3-Bllossom"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

pipeline.model.eval()

PROMPT = '''ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆì˜ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.'''
instruction = "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ MLPì—°êµ¬ì‹¤ì— ëŒ€í•´ ì†Œê°œí•´ì¤˜"

messages = [
    {"role": "system", "content": f"{PROMPT}"},
    {"role": "user", "content": f"{instruction}"}
    ]

prompt = pipeline.tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = pipeline(
    prompt,
    max_new_tokens=2048,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
    repetition_penalty = 1.1
)

print(outputs[0]["generated_text"][len(prompt):])

# ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ MLPì—°êµ¬ì‹¤ì€ ë©€í‹°ëª¨ë‹¬ ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤. êµ¬ì„±ì›ì€ ì„ê²½íƒœ êµìˆ˜ì™€ ê¹€ë¯¼ì¤€, ê¹€ìƒë¯¼, ìµœì°½ìˆ˜, ì›ì¸í˜¸, ìœ í•œê²°, ì„í˜„ì„, ì†¡ìŠ¹ìš°, ìœ¡ì •í›ˆ, ì‹ ë™ì¬ í•™ìƒì´ ìˆìŠµë‹ˆë‹¤.
```

### Python code with AutoModel
```python

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = 'MLP-KTLim/llama3-Bllossom'

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

model.eval()

PROMPT = '''ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆì˜ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.'''
instruction = "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ MLPì—°êµ¬ì‹¤ì— ëŒ€í•´ ì†Œê°œí•´ì¤˜"

messages = [
    {"role": "system", "content": f"{PROMPT}"},
    {"role": "user", "content": f"{instruction}"}
    ]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = model.generate(
    input_ids,
    max_new_tokens=2048,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
    repetition_penalty = 1.1
)

print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))
# ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ MLPì—°êµ¬ì‹¤ì€ ë©€í‹°ëª¨ë‹¬ ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤. êµ¬ì„±ì›ì€ ì„ê²½íƒœ êµìˆ˜ì™€ ê¹€ë¯¼ì¤€, ê¹€ìƒë¯¼, ìµœì°½ìˆ˜, ì›ì¸í˜¸, ìœ í•œê²°, ì„í˜„ì„, ì†¡ìŠ¹ìš°, ìœ¡ì •í›ˆ, ì‹ ë™ì¬ í•™ìƒì´ ìˆìŠµë‹ˆë‹¤.
```



## Citation
**Language Model**
```text
@misc{bllossom,
  author = {ChangSu Choi, Yongbin Jeong, Seoyoon Park, InHo Won, HyeonSeok Lim, SangMin Kim, Yejee Kang, Chanhyuk Yoon, Jaewan Park, Yiseul Lee, HyeJin Lee, Younggyun Hahm, Hansaem Kim, KyungTae Lim},
  title = {Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean},
  year = {2024},
  journal = {LREC-COLING 2024},
  paperLink = {\url{https://arxiv.org/pdf/2403.10882}},
 },
}
```

**Vision-Language Model**
```text
@misc{bllossom-V,
  author = {Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim},
  title = {X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment},
  year = {2024},
  publisher = {GitHub},
  journal = {NAACL 2024 findings},
  paperLink = {\url{https://arxiv.org/pdf/2403.11399}},
 },
}
```

## Contact
 - ì„ê²½íƒœ(KyungTae Lim), Professor at Seoultech. `ktlim@seoultech.ac.kr`
 - í•¨ì˜ê· (Younggyun Hahm), CEO of Teddysum. `hahmyg@teddysum.ai`
 - ê¹€í•œìƒ˜(Hansaem Kim), Professor at Yonsei. `khss@yonsei.ac.kr`

## Contributor
 - ìµœì°½ìˆ˜(Chansu Choi), choics2623@seoultech.ac.kr
 - ê¹€ìƒë¯¼(Sangmin Kim), sangmin9708@naver.com
 - ì›ì¸í˜¸(Inho Won), wih1226@seoultech.ac.kr
 - ê¹€ë¯¼ì¤€(Minjun Kim), mjkmain@seoultech.ac.kr 
 - ì†¡ìŠ¹ìš°(Seungwoo Song), sswoo@seoultech.ac.kr
 - ì‹ ë™ì¬(Dongjae Shin), dylan1998@seoultech.ac.kr
 - ì„í˜„ì„(Hyeonseok Lim), gustjrantk@seoultech.ac.kr
 - ìœ¡ì •í›ˆ(Jeonghun Yuk), usually670@gmail.com
 - ìœ í•œê²°(Hangyeol Yoo), 21102372@seoultech.ac.kr
 - ì†¡ì„œí˜„(Seohyun Song), alexalex225225@gmail.com
